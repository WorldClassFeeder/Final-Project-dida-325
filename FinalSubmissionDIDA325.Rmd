---
title: 'Final Project: Riya, Daniel, Andrew, Maya, Lulu'
output:
  html_document:
    df_print: paged
---
# 1. Introduction 
Our team is positioned as analysts working for the US Government. We are tasked with supporting the FBI. The goal of our efforts is to help maximize crime clearance rate by identifying potential patterns in which crimes are more likely to be solved. We can do this by understanding trends and providing actionable insights that improve investigative efficiency and resource allocation across law enforcement agencies in the US.

This dataset originates from research conducted by The Marshall Project, which is a nonprofit organization dedicated to highlighting the national urgency surrounding the US criminal justice system. They analyzed data from the FBI's uniform crime reporting program to determine which types of crimes are most likely to be solved. Additionally, they examined the number of crimes reported by agencies and the number of cases solved each year.

The columns ori and agency_name are text identifiers for each police agency. State is categorical, year is numeric, and “fips_state_county_code” and “number_of_months_missing” are numeric. The “new_” columns are numeric counts of reported crimes, and the corresponding “_cleared” columns are numeric counts of solved crimes. The dataset also includes numeric totals such as violent_crime, property_crime, total_crimes. Lastly, all “clearance_rate_” columns are numeric percentages showing the share of cases solved for each crime type and for crime overall.

Q1: How have clearance rates for violent crimes vs. property crimes changed over time in 5 different states with the largest population, and which states show the biggest improvement or decline? (Visualization)

Q2: Does New York differ from other states in its clearance rates for all types of crime over time? (visualization)

Q3: Can overall clearance rate be predicted using crime volume variables like number of assaults, robberies, burglaries, thefts, and motor vehicle thefts? (Linear Regression)

Q4: Can we classify whether a specific crime category (e.g., murder, robbery, burglary) will have a “high” vs. “low” clearance rate based on crime totals and state/year characteristics? (Logistic Regression or Random Forest). A high clearance rate being 65% and above.

# 2. Dataset 
To prepare the dataset for our four research questions, we removed all variables that did not directly relate to changes in clearance rates, differences between states, or prediction/classification of overall clearance rate.

### Removed Variables
**1. Identification variables (not useful for analysis)**
- `ori`, `agency_name`, `fips_state_county_code`
These identify agencies but do not contribute to crime patterns.

**2. Cleared-case count variables**
- All `cleared_*` columns
We use clearance *rates* instead of counts, so these are redundant and
introduce information leakage.

**3. Crime types not included in our research questions**
- `new_murder`, `new_manslaughter`, `new_rape`, `new_arson`,
  `new_total_crime`
Our focus is specifically on assault, robbery, burglary, theft, and motor
vehicle theft. We are also focusing on clearance rate differences between 
violent and property crime.

**4. Other clearance rate variables**
- All `clearance_rate_*` variables except `clearance_rate_total_crime`, 
`clearance_rate_violent_crime`, and `clearance_rate_property_crime`
These represent unrelated crime categories or alternate target variables.

### Variables Kept
We kept only variables relevant to our questions:
- `state`, `year`
- `new_assault`, `new_robbery`, `new_burglary`, `new_theft`,
  `new_motor_vehicle_theft`
  `clearance_rate_violent_crime`,`clearance_rate_property_crime`
- `clearance_rate_total_crime` (our main response variable)

### Loading Libraries and Data 
```{r}
library(dplyr)
library(ggplot2)
library(RCurl)
library(tidyr)

data<-getURL("https://media.githubusercontent.com/media/themarshallproject/crime-clearance-rate/refs/heads/main/crime_clearance_rate.csv")
crime<-read.csv(text=data)

```
### Filtered Dataset
```{r}
crime_filtered <- select(
  crime,
  state, year,
  new_assault, new_robbery, new_burglary, new_theft, new_motor_vehicle_theft,
  clearance_rate_total_crime, clearance_rate_violent_crime, clearance_rate_property_crime
)

crime_top5pop <- crime_filtered %>%
  filter(state %in% c("new york", "california", "texas", "florida", "pennsylvania"))

crime_top5pop2020 <- crime_filtered %>%
  filter(state %in% c("new york", "california", "texas", "florida", "pennsylvania"), year == 2020)

crime_clean <- crime_top5pop %>%
  group_by(state, year) %>%
  mutate(violent_avg  = mean(clearance_rate_violent_crime, na.rm = TRUE),
         property_avg = mean(clearance_rate_property_crime, na.rm = TRUE))

crime_clean$violent_avg[is.infinite(crime_clean$violent_avg)] <- NA
crime_clean$property_avg[is.infinite(crime_clean$property_avg)] <- NA

crime_clean <- crime_clean |> 
  dplyr::filter(!is.na(violent_avg), !is.na(property_avg)) 

#Cite:https://stackoverflow.com/questions/28857653/removing-na-observations-with-dplyrfilter
``` 

# Analysis
```{r}
crime_filtered %>% head(10)

```
# 2.1. Question 1 
```{r}
#Q1: How have clearance rates for violent crimes vs. property crimes changed over time in 5 different states with the largest population, and which states show the biggest improvement or decline? (Visualization)


# Violent crime clearance over time 
ggplot(crime_clean,aes(x=year,y=violent_avg,color=state))+
  geom_line(size=.8)+
  geom_point(size=1) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  labs(title = "Violent crime clearance rates over time\nTop 5 most populated states", x = "Year",y = "Average clearance rate for violent crime (%)")

#Property crime clearance rates over time 
ggplot(crime_clean,aes(x=year,y=property_avg,color=state))+
  geom_line(size=.8)+
  geom_point(size=1) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +
  theme_minimal()+
  labs(title = "Property crime clearance rates over time\nTop 5 most populated states", x = "Year",y = "Average clearance rate for Property crime (%)")

#Cite:https://ggplot2.tidyverse.org/reference/expansion.html
```

The line plots show how often police solved violent and property crimes from 1970 through 2020 in the five most populated states. New York keeps the highest success rate for both crime types. For violent crime it stays between 65 plus 75 percent every year. Pennsylvania ranks second and both states solve property crimes repeatedly than the other three.

California ranks last in both categories for the whole fifty year span - about half of violent crimes but also only ten to fifteen percent of property crimes are cleared. Texas shows a clear slide in violent crime clearance falling from the low seventies in the 1970s to the mid fifties by 2020.

Florida's line jumps up and down, especially in the 1990s. The sharp rises as well as falls occur because Florida left multiple years blank - the gaps pull the yearly averages down and then up. Any statement about Florida must take those empty years into account.

In every state the violent crime clearance rate stays higher or steadier than the property crime rate. Property-crime clearance stays low everywhere - no state reaches thirty percent in most years. The patterns indicate that state level policing methods, the steadiness of reporting and the resources given to agencies shape clearance results more than population size alone.

#2.2. Question 2 
```{r}
#Q2: Does New York differ from other states in its clearance rates for all types of crime over time? (visualization)

crime_ny_ca <- crime_filtered %>%
  drop_na(clearance_rate_total_crime) %>%
  filter(state %in% c("new york","california")) %>%
  filter(is.finite(clearance_rate_total_crime)) %>%
  group_by(state,year) %>%
  summarize(avg_clearance_rate = mean(clearance_rate_total_crime))

ggplot(crime_ny_ca,aes(x=year,y=avg_clearance_rate,color=factor(state),group=state)) + 
  geom_line(size=1) + 
  geom_point(size=2) +
  theme_minimal() +
  labs(x="Year",y="Clearance Rate",title="Crime Clearance Rate in New York vs California by Year",color="State") +
  theme(legend.position="bottom") +
  scale_y_continuous(labels=scales::comma,limits=c(0,40)) +
  scale_color_manual(values=c("#FFB900","#5773CC"))
```
The line chart displays how California's crime clearance rate differs from New York's crime clearance clearance rate from 1970 to 2020. New York consistently has better clearance rates in every year in comparison to California. 

# 2.3. Question 3 
```{r}
#filtering data for q3 and q4
#A clean outcome variable (clearance_rate_total_crime)
#No NA, Inf, or negative values in that column

crime_clean1 <- crime %>%
  filter(
    is.finite(clearance_rate_total_crime),
    !is.na(clearance_rate_total_crime),
    clearance_rate_total_crime >= 0,
    clearance_rate_total_crime <= 100
  )
nrow(crime_clean1)

crime_q3 <- crime_clean1 %>%
  select(
    clearance_rate_total_crime,
    new_assault, new_robbery, new_burglary,
    new_theft, new_motor_vehicle_theft
  ) %>%
  filter(complete.cases(.))

```

```{r}
#question 3 model

model3 <- lm(
  clearance_rate_total_crime ~
    new_assault + new_robbery + new_burglary +
    new_theft + new_motor_vehicle_theft,
  data = crime_q3
)

summary(model3)
```


```{r}
# Q3: Visual
crime_q3$predicted_clearance <- predict(model3)

ggplot(crime_q3, aes(x = predicted_clearance, 
                     y = clearance_rate_total_crime)) +
  geom_point(alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(
    x = "Predicted clearance rate (%)",
    y = "Actual clearance rate (%)",
    title = "Predicted vs. actual clearance rate\nLinear regression model (Q3)"
  )

```

We built a straight line model that uses five crime counts: new_assault, new_robbery, new_burglary, new_theft, and new_motor_vehicle_theft - to predict the overall clearance rate. The model returned an R-squared of 0.001724; therefore, crime volume accounts for roughly 0.17 percent of the differences in clearance rates. All five predictors reached statistical significance (p < 2e-16) - yet that outcome stems mainly from the huge sample of more than 660 000 records. The slope values are tiny - one extra reported assault links to a rise of only 0.00027 percentage points in the clearance rate - the real world impact of crime counts on clearance rates is slight.

Although the model passes the test for statistical significance, the crime volume variables do not predict the overall clearance rate with any strength. The figures show that factors like the money and staff an agency commands, its policies plus the local setting almost certainly shape clearance rates far more than the simple count of reported crimes.

Visual assessment supports this conclusion because the predicted-versus-actual plot shows that almost all predictions stay within a narrow band of about twenty percent. The true clearance rates spread from zero to one hundred percent. A number of predictions lie below zero, which is impossible this shows that the model is misspecified. Because the points do not lie on the 1:1 reference line, the plot confirms that the linear model predicts poorly.

```{r}
# High clearance = 65% or more
crime_clean1$high_clearance <- ifelse(
  crime_clean1$clearance_rate_total_crime >= 65, 1, 0
)

# Build dataset for Q4
crime_q4 <- crime_clean1 %>%
  select(
    high_clearance,
    new_assault, new_robbery, new_burglary,
    new_theft, new_motor_vehicle_theft,
    state, year
  ) %>%
  filter(complete.cases(.))

crime_q4$state <- as.factor(crime_q4$state)

nrow(crime_q4)
```

# 2.4. Question 4
```{r}
#question 4 model
model4 <- glm(
  high_clearance ~ .,
  data = crime_q4,
  family = "binomial"
)

summary(model4)
#heads up this takes like 3-5 minutes to load its a lot of data


```

```{r}
# Q4: Visualizing logistic regression predictions

# Turn outcome into a factor (0 vs 1)
crime_q4$high_clearance <- as.factor(crime_q4$high_clearance)

# Get predicted probabilities from the logistic model
crime_q4$pred_prob <- predict(model4, type = "response")

# Boxplot of predicted probabilities by actual outcome
ggplot(crime_q4, aes(x = high_clearance, y = pred_prob)) +
  geom_boxplot() +
  labs(
    x = "High clearance (0 = <65%, 1 = ≥65%)",
    y = "Predicted probability of high clearance",
    title = "Predicted probabilities by outcome\nLogistic regression model (Q4)"
  )

```
To classify whether an agency-year had a high clearance rate (defined as for an overall clearance rate of at least 65%, we fit a logistic regression model using crime volume predictors (`new_assault`, `new_robbery`,`new_burglary`, `new_theft`, and `new_motor_vehicle_theft`) with `state` and `year` included as additional predictors. The dataset was cleaned to remove missing or infinite values, and a binary outcome variable `high_clearance` was created.

The model results indicate that all five of the crime volume predictors were statistically significant (p < 0.001). Higher numbers of assaults and robberies were associated with higher odds of achieving a high clearance rate, while higher levels of burglaries, thefts, and
motor-vehicle thefts were associated with lower odds. Although these effects were statistically significant, the magnitudes of the coefficients were extremely small, indicating that crime volume has limited practical influence on clearance results.

Most state indicator variables were not significant. This implies that, after controlling for crime volume, differences between states do not strongly predict whether an agency achieves a high clearance rate. A few states, including Michigan, New Hampshire, New Jersey, and Washington D.C. did show significant effects relative to the reference state.

The null deviance dropped from 237,757 to 203,170, meaning that the model improves on a baseline classifier. The AIC of 203,292 provides a benchmark with which future models can be compared. Overall, the predictive strength is moderate.

In sum, logistic regression suggested that crime volumes and year have statistically measurable effects on the probability of achieving a high clearance rate, but these predictors alone are not strong enough to make highly accurate classifications. Additional information—such as staffing levels, funding, community characteristics, or agency resources would probably improve the classification performance.

Visual Graph Interpretation:
The forecast probabilities for high clearance pile up in the same zone for both sets of agencies - the logistic model fails to separate those below the 65 % mark from those above it. The picture confirms that crime volume figures carry little weight when one tries to predict whether a case will be cleared.

# 3. Conclusion 

Our analysis shows that violent crime clearance rates are consistently higher than property crime clearance rates across the 5 most populated states, with New York and Pennsylvania performing the best, while California and Texas were behind. While violent crime clearance rate has remained relatively stable over time, property crime clearance rates remain low in all states, rarely exceeding 30%. Such differences between states suggest that something other than crime volume is impacting clearance rates, such as structural or contextual factors.

When comparing New York and California, New York consistently maintains higher overall clearance rates throuought the 1970-2020 period. Predictive modeling using crime volume variables indicates that while statistically significant, these factors explain almost none of the variation in overall clearance rates. This highlights the limited practical impact of raw crime counts. Logistic regression analysis for classifying high (>= 65%) vs. low clearance rates shows modest predictive power, as crime volume, year, and state had measurable but small effects. This continues to further the idea that structural and contextual factors likely drive clearance outcomes more than crime counts alone.

Limitations of this study include missing data and variations in reporting practices. Both of these factors, along with others, may impact these results. Future analysis would benefit from enriching the dataset with more contextual information. This could include information on staffing, budgets, and community information to provide context to this data. Agencies should focus on interventions that improve clearance rates, and potentially modeling their systems after those that are most successful in similar contexts. While our analysis provides some insights into crime clearance rates, more data and sophisticated modeling are necessary to guide any reccomendations for improving clearance rates.




## Citations
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html used for question 3 model using the lm function
https://ggplot2.tidyverse.org/reference/ mainly for q3 ggplot() ,geom_point(), geom_abline(), labs(). Used this one a lot
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html used for question 4 model using glm function
https://stackoverflow.com/questions/28857653/removing-na-observations-with-dplyrfilter, used for filering crime_clear
https://ggplot2.tidyverse.org/reference/expansion.html, used for Q1
